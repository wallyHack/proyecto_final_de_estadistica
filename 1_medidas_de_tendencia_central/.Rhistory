points(187, 95, pch=16, col="red")
lines(c(187, 187), c(92.3014, 95), col="black")
#Punto (180 - 96)
points(180, 96, pch=16, col="red")
lines(c(180, 180), c(82.9389, 96), col="black")
#Punto (170 - 65)
points(170, 65, pch=16, col="red")
lines(c(170, 170), c(69.5639, 65), col="black")
#Punto (175 - 70)
points(175, 70, pch=16, col="red")
lines(c(175, 175), c(76.2514, 70), col="black")
#Punto (165 - 65)
points(165, 65, pch=16, col="red")
lines(c(165, 165), c(62.8764, 65), col="black")
#Punto (173 - 90)
points(173, 90, pch=16, col="red")
lines(c(173, 173), c(73.5764, 90), col="black")
#Punto (180 - 70)
points(180, 70, pch=16, col="red")
lines(c(180, 180), c(82.9389, 70), col="black")
#Punto (170 - 68)
points(170, 68, pch=16, col="red")
lines(c(170, 170), c(69.5639, 68), col="black")
#						CORRELACION
#Ingresar Datos De X
x <- c(170, 173, 168, 180, 168, 172, 174, 180, 170, 165, 178, 187, 163, 170, 179)
#Ingresar Datos De Y
y <- c(62, 84, 80, 70, 65, 90, 70, 96, 65, 65, 68, 95, 61, 68, 78)
#Promedio De X
mean(x)
#Promedio De Y
mean(y)
#Desviacion Estandar de X
sd(x)
#Desviacion Estandar de Y
sd(y)
#Correlacioon
cor.test(x, y)
#Graficando Correlacion
plot(x, y, col="blue", main="Medidas De Dispersion", xlab="Representacion De Datos De Muestra De X", ylab="Representacion De Datos De Muestra De Y")
#Linea De Referencia
abline(lsfit(x, y))
#Distribucion Normal
#calculando la probabilidad de si un evento es mayor o menor a un valor
#P(x> o x<N)
#|----Tabla PSP----|
x <- c(300,335,489,120,66,98,452) #LOCS
#x
y <- c(9.8,8.9,12,4,1.8,3.2,13.2) # HOURS
#y
#media x
media_X <- mean(x)
media_X
#media y
media_Y <- mean(y)
media_Y
#sd x
sd_X <- sd(x)
sd_X
#sd y
sd_Y <- sd(y)
sd_Y
#correlaci?n
correlacion <- cor(x,y)
correlacion
#normalizando en z
# Z = (x - Media)/varianza
#numero <- scan()
z <- (493 - media_X)/sd_X
z
#NOTA: Cuando es x<78 se busca el complemento
#probabilidad de distribuci?n seg?n la Tabla Normal Z
#NOTA: Para confirmar buscar el valor de z en la Tabla Z
#probabilidad <- pnorm(z, lower.tail = TRUE)# <=
probabilidad <- pnorm(z, lower.tail = FALSE)*100# >
probabilidad
#b1 r(Sy/Sx)
beta1 <- (correlacion)*(sd_Y/sd_X)
beta1
#b0 Media y - b1Media x
beta0 <- (media_Y)-(beta1*media_X)
beta0
#h(x) = b0 + b1(x)
h_x <- beta0+(beta1*78)
h_x
#Grafica
cord.x <- c(1.16, seq(1.16,2.15,0.01), 2.15)
cord.y <- c(0, dnorm(seq(1.16,2.15,0.01)), 0)
curve(dnorm(x,0,1), xlim = c(-3,3), main="Distribucion Normal")
polygon(cord.x, cord.y, col = "skyblue")
#abline(v=74.3)
#eliminacion de objetos
rm(x, y, media_X, media_Y, sd_X, sd_Y, correlacion, z, probabilidad, beta0, beta1, h_x)
#					Distribucion Normal
#Ingresar Datos De X
x <- c(66, 61, 96, 90, 65, 96, 65, 70, 70, 64)
x
#Promedio De X
mean(x)
#Media De X
median(x)
#Desviacion Estandar De X
sd(x)
#Normaliza Datos De X
dnorm(x,mean(x),sd(x),log=F)
#Histograma De X
hist(x, freq = F)
#Se Agrega La Curva A Hitograma con sus respectivos rangos
curve(dnorm(x,mean(x),sd(x)),from = 60, to = 96, add = T)
#Ingresar Datos De Y
y <- c(175, 163, 187, 173, 173, 180, 165, 175, 180, 170)
y
#Promedio De Y
mean(y)
#Media De Y
median(y)
#Desviacion Estandar De Y
sd(y)
#Normaliza Datos De Y
dnorm(y,mean(y),sd(y),log=F)
#Histograma De Y
hist(y, freq = F)
#Se Agrega La Curva A Hitograma con sus respectivos rangos
curve(dnorm(y,mean(y),sd(y)),from = 160, to = 190, add = T)
#Correlacion
correlacion <- cor.test(x, y)
correlacion
#Ajustar Modelo
z <- lm(y ~ x)
#Resumen De Modelo
summary(z)
#Calcular z
z <- (68-mean(x))/sd(x)
z
#Normal
pnorm(68, mean = 74.3, sd = 13.9, lower.tail = F)
pnorm(68, mean = 74.3, sd = 13.9, lower.tail = F)*100
#					Distribucion T Student
#Datos Aleatorios
#					t <- rnorm(16, 13, 5.6)
#Valores Generados
#					t
#Calculo T Student
#					t.test(t)
#hist(t, col="blue")
#Distribucion T-Student
#Datos Aleatorios
t <- runif(16, 1, 10)
t
#Registros En Total
n <- length(t)
n
#Promedio
Promedio <- mean(t)
Promedio
#Desviaci?n Estandar
sd=5.6
Desviacion_Estandar <- sd
Desviacion_Estandar
#Media
mean=13
Media <- mean
Media
#Grados De Libertad
Grados_Libertad <- n-1
Grados_Libertad
#Intevalo De Confianza
Int_Confianza <- 99
Int_Confianza
#Riesgo
Riesgo <- 100 - Int_Confianza
Riesgo
#Alfa
Alfa <- 1- ((Riesgo/100)/2)
Alfa
#Valor Criticos(tCr?tico)
ValorCritico <- qt(Alfa, Grados_Libertad, lower.tail = TRUE)# <=
ValorCritico
qt(Alfa, Grados_Libertad, lower.tail = FALSE)# >
#Valores Extremos
#Obtener Regi?n Cr?tica Inferior y Superior
Region_Critica_Superior <- ((Promedio)+(ValorCritico*Desviacion_Estandar)/(sqrt(n)))
Region_Critica_Superior
Region_Critica_Inferior <- ((Promedio)-(ValorCritico*Desviacion_Estandar)/(sqrt(n)))
Region_Critica_Inferior
# 					Regresion Multiple
#Conjunto De Datos
#x1 <-c(1,1,1,1)
#x2 <-c(3,5,2,1)
#x3 <-c(3,4,7,8)
#b1 <-c(1,3,9,9)
x1 <- c(1,1,1,1,1,1)
x2 <- c(1,2,3,4,5,6)
b1 <- c(0.20, 0.25, 0.20, 0.35, 0.45, 0.40)
#Formula
#(Regresion = lm(b1 ~ x1 + x2 + x3))
(Regresion = lm(b1 ~ x1 + x2))
#Resumen De La Regresion Con Funcion Summary
summary(Regresion)
#La Varianza Del Modelo Se Obtiene Mediante El Comando Anova:
anova(Regresion)
#Distribucion T-Student
#con datos aleatorios
#t <- rnorm(16, 13, 5.6)#se generan los numeros de forma aleatoria
#t
#t.test(t)
#Horas
#x <- c(9.8, 8.9, 12, 4, 1.8, 3.2, 13.2)
#x
#LOC?S
x <- c(300, 335, 489, 120, 96, 98, 452)
x
#total de registros
n <- length(x)
n
#media(promedio)
media_X <- mean(x)
media_X
#desviaci?n estandar
sd_X <- sd(x)
sd_X
#grados de libertad
GL <- n-1
GL
#intevalo de confianza  -> Es lo unico que se modifica en este PROGRAMA
int_Confianza <- 90
#riesgo
riesgo <- 100 - int_Confianza
riesgo
#alfa
alfa <- 1- ((riesgo/100)/2)
alfa
#valores criticos(tCr?tico) seg?n la tabla de Distribuci?n T-Student
#Nota: Para confirmar buscar en la taba de Distribucion t -Student
valorCritico <- qt(alfa, GL, lower.tail = TRUE)# <=
valorCritico
qt(alfa, GL, lower.tail = FALSE)# >
#formula de X valores extremos
#para obtener regi?n cr?tica inferior y superior
region_critica_superior <- ((media_X)+(valorCritico*sd_X)/(sqrt(n)))
region_critica_superior
region_critica_inferior <- ((media_X)-(valorCritico*sd_X)/(sqrt(n)))
region_critica_inferior
#eliminacion de objectos
rm(x, n, media_X, sd_X, GL, int_Confianza, riesgo, alfa, valorCritico, region_critica_inferior, region_critica_superior)
#Grafica
cord.x <- c(1.20, seq(1.20,13.90,0.01), 13.90)
cord.y <- c(0, dnorm(seq(1.20,13.90,0.01)), 0)
curve(dnorm(x,0,1), xlim = c(-5,15), main="Distribucion Normal")
polygon(cord.x, cord.y, col = "skyblue")
# 					Naive Bayes
#install.packages("e1071")
#Libreria Cargada
library(e1071)
#Conjunto De Datos
Sexo = c("H","H","H","H","M","M","M","M")
Altura = c(6,5.72,5.58,5.92,5,5.5,5.42,5.75)
Peso = c(180,190,170,165,100,150,130,150)
Calzado = c(12,11,12,10,6,8,7,9)
Medidas = data.frame(Altura,Peso,Calzado,Sexo)
#Mostrar Resultado
Medidas
#Resumen Del Resultado
summary(Medidas)
#Clasificando elementos
classifier=naiveBayes(Medidas[,1:3], Medidas[,4])
#Mostrando Clasificaci?n En Tabla
table(predict(classifier, Medidas[,-4]), Medidas[,4])
install.packages("e1071")
install.packages("e1071")
# 					Naive Bayes
install.packages("e1071")
#Libreria Cargada
library(e1071)
#Conjunto De Datos
Sexo = c("H","H","H","H","M","M","M","M")
Altura = c(6,5.72,5.58,5.92,5,5.5,5.42,5.75)
Peso = c(180,190,170,165,100,150,130,150)
Calzado = c(12,11,12,10,6,8,7,9)
Medidas = data.frame(Altura,Peso,Calzado,Sexo)
#Mostrar Resultado
Medidas
#Resumen Del Resultado
summary(Medidas)
#Clasificando elementos
classifier=naiveBayes(Medidas[,1:3], Medidas[,4])
#Mostrando Clasificaci?n En Tabla
table(predict(classifier, Medidas[,-4]), Medidas[,4])
#algorimtmo K-means
#vector S
S <- c(44, 48, 38, 39, 45, 50, 40, 35, 45, 40, 45)
#vector L
L <- c(55, 59, 55, 40, 45, 55, 55, 40, 40, 45, 50)
#SC1
SC1 <- 41.2
#LC1
LC1 <- 47.2
#SC2
SC2 <- 49
#LC2
LC2 <- 57
#DC1
DC1 <- sqrt(((S-SC1)^2)+((L-LC1)^2))
DC1
#DC2
DC2 <- sqrt(((S-SC2)^2)+((L-LC2)^2))
DC2
# P <- c(2.1, 3.7, 3.1, 2.1, 2.3, 3.4, 2.7, 3, 3.1)
# R <- c(4, 8, 16, 8, 4, 12, 4, 6, 8)
# D <- c(256, 512, 1024, 500, 256, 512, 256, 256, 256)
#
# #centroide 1
# PC1 <- 2.64
# RC1 <- 5.2
# DC1 <- 256
#
# #centroide 2
# PC2 <- 2.1
# RC2 <- 8
# DC2 <- 500
#
# #centroide 3
# PC3 <- 3.4
# RC3 <- 12
# DC3 <- 682.66
#
# DC1 <- sqrt(((P-PC1)^2)+((R-RC1)^2)+(D-DC1)^2)
# DC1
#
# DC2 <- sqrt(((P-PC2)^2)+((R-RC2)^2)+(D-DC2)^2)
# DC2
#
# DC3 <- sqrt(((P-PC3)^2)+((R-RC3)^2)+(D-DC3)^2)
# DC3
ls()
rm(PC1, RC1, DC1, PC2, RC2, DC2, PC3, RC3, DC1, DC2, DC3)
#algoritmo usando un paquete
install.packages("nayvebayes")
data(iris)
nb <- naive_bayes(Species ~ ., data = iris)
plot(nb)
nb_kernel <- naive_bayes(x = iris[-5], y = iris[ ,5], usekernel = TRUE)
plot(nb_kernel)
install.packages("nayvebayes")
#library(MASS)
x = matrix(
c(44, 55, 48, 59, 38, 55, 39, 40, 45, 45, 50, 55, 40, 55, 35, 40, 45, 40, 40, 45, 45, 50), # the data elements
nrow=11,
ncol=2,
byrow = TRUE)
#Conjunto De Datos
x
#Graficando x
plot(x)
#Objeto Guardado En Datos
datos <- rbind(x)
#Graficando Objeto Guardado
plot(datos)
#Aplicando - Graficando Kmeans
resultado<-kmeans(datos, centers=2, iter.max = 2)
plot(datos, col=resultado$cluster, main="K - Means", xlab="Representaci?n De Datos De Conjunto De Datos", ylab="Frecuencia", xlim=c(35,60), ylim=c(35, 60))
#Centroides De Conjunto De Datos
points(resultado$centers, cex=2, col=10, pch=19)
# 					Naive Bayes
install.packages("e1071")
#Libreria Cargada
library(e1071)
#Conjunto De Datos
Sexo = c("H","H","H","H","M","M","M","M")
Altura = c(6,5.72,5.58,5.92,5,5.5,5.42,5.75)
Peso = c(180,190,170,165,100,150,130,150)
Calzado = c(12,11,12,10,6,8,7,9)
Medidas = data.frame(Altura,Peso,Calzado,Sexo)
#Mostrar Resultado
Medidas
#Resumen Del Resultado
summary(Medidas)
#Clasificando elementos
classifier=naiveBayes(Medidas[,1:3], Medidas[,4])
#Mostrando Clasificaci?n En Tabla
table(predict(classifier, Medidas[,-4]), Medidas[,4])
install.packages("e1071")
# 					Naive Bayes
install.packages("e1071")
#Libreria Cargada
library(e1071)
#Conjunto De Datos
Sexo = c("H","H","H","H","M","M","M","M")
Altura = c(6,5.72,5.58,5.92,5,5.5,5.42,5.75)
Peso = c(180,190,170,165,100,150,130,150)
Calzado = c(12,11,12,10,6,8,7,9)
Medidas = data.frame(Altura,Peso,Calzado,Sexo)
#Mostrar Resultado
Medidas
#Resumen Del Resultado
summary(Medidas)
#Clasificando elementos
classifier=naiveBayes(Medidas[,1:3], Medidas[,4])
#Mostrando Clasificaci?n En Tabla
table(predict(classifier, Medidas[,-4]), Medidas[,4])
# 					Regresion Multiple
#Conjunto De Datos
#x1 <-c(1,1,1,1)
#x2 <-c(3,5,2,1)
#x3 <-c(3,4,7,8)
#b1 <-c(1,3,9,9)
#x1 <- c(3,3,4,4,3,2,4,4)
#x2 <- c(3,2,1,4,2,2,2,3)
#x3 <- c(2,1,2,3,2,2,2,3)
#b1 <- c(1,2,3,4,1,2,3,4)
x1 <- c(0.792,0.896,0.938,0.250,0.667,0.979,0.500,0.084)
x2 <- c(0.125,0.333,0.396,0.667,0.083,0.042,0.271,0.646)
x3 <- c(0.021,0.417,0.438,0.688,0.042,0.125,0.146,0.625)
x4 <- c(0.896,0.688,0.625,0.354,0.938,0.979,0.729,0.375)
#Formula
(Regresion = lm(b1 ~ x1 + x2 + x3 + x4))
#Resumen De La Regresion Con Funcion Summary
summary(Regresion)
#La Varianza Del Modelo Se Obtiene Mediante El Comando Anova:
anova(Regresion)
# 					Naive Bayes
#install.packages("e1071")
#Libreria Cargada
library(e1071)
#Conjunto De Datos
Sexo = c("H","H","H","H","M","M","M","M")
Altura = c(6,5.72,5.58,5.92,5,5.5,5.42,5.75)
Peso = c(180,190,170,165,100,150,130,150)
Calzado = c(12,11,12,10,6,8,7,9)
Medidas = data.frame(Altura,Peso,Calzado,Sexo)
#Mostrar Resultado
Medidas
#Resumen Del Resultado
summary(Medidas)
#Clasificando elementos
classifier=naiveBayes(Medidas[,1:3], Medidas[,4])
#Mostrando Clasificaci?n En Tabla
table(predict(classifier, Medidas[,-4]), Medidas[,4])
# 					Naive Bayes
#install.packages("e1071")
#Libreria Cargada
library(e1071)
#Conjunto De Datos
Sexo = c("H","H","H","H","M","M","M","M")
Altura = c(6,5.72,5.58,5.92,5,5.5,5.42,5.75)
Peso = c(180,190,170,165,100,150,130,150)
Calzado = c(12,11,12,10,6,8,7,9)
Medidas = data.frame(Altura,Peso,Calzado,Sexo)
#Mostrar Resultado
Medidas
#Resumen Del Resultado
summary(Medidas)
#Clasificando elementos
classifier=naiveBayes(Medidas[,1:3], Medidas[,4])
#Mostrando Clasificaci?n En Tabla
table(predict(classifier, Medidas[,-4]), Medidas[,4])
# 					Naive Bayes
#install.packages("e1071")
#Libreria Cargada
library(e1071)
#Conjunto De Datos
Sexo = c("H","H","H","H","M","M","M","M")
Altura = c(6,5.72,5.58,5.92,5,5.5,5.42,5.75)
Peso = c(180,190,170,165,100,150,130,150)
Calzado = c(12,11,12,10,6,8,7,9)
Medidas = data.frame(Altura,Peso,Calzado,Sexo)
#Mostrar Resultado
Medidas
#Resumen Del Resultado
summary(Medidas)
#Clasificando elementos
classifier=naiveBayes(Medidas[,1:3], Medidas[,4])
#Mostrando Clasificaci?n En Tabla
table(predict(classifier, Medidas[,-4]), Medidas[,4])
#library(MASS)
x = matrix(
c(44, 55, 48, 59, 38, 55, 39, 40, 45, 45, 50, 55, 40, 55, 35, 40, 45, 40, 40, 45, 45, 50), # the data elements
nrow=11,
ncol=2,
byrow = TRUE)
#Conjunto De Datos
x
#Graficando x
plot(x)
#Objeto Guardado En Datos
datos <- rbind(x)
#Graficando Objeto Guardado
plot(datos)
#Aplicando - Graficando Kmeans
resultado<-kmeans(datos, centers=2, iter.max = 2)
plot(datos, col=resultado$cluster, main="K - Means", xlab="Representaci?n De Datos De Conjunto De Datos", ylab="Frecuencia", xlim=c(35,60), ylim=c(35, 60))
#Centroides De Conjunto De Datos
points(resultado$centers, cex=2, col=10, pch=19)
#library(MASS)
x = matrix(
c(44, 55, 48, 59, 38, 55, 39, 40, 45, 45, 50, 55, 40, 55, 35, 40, 45, 40, 40, 45, 45, 50), # the data elements
nrow=11,
ncol=2,
byrow = TRUE)
#Conjunto De Datos
x
#Graficando x
plot(x)
#Objeto Guardado En Datos
datos <- rbind(x)
#Graficando Objeto Guardado
plot(datos)
#Aplicando - Graficando Kmeans
resultado<-kmeans(datos, centers=2, iter.max = 2)
plot(datos, col=resultado$cluster, main="K - Means", xlab="Representaci?n De Datos De Conjunto De Datos", ylab="Frecuencia", xlim=c(35,60), ylim=c(35, 60))
#Centroides De Conjunto De Datos
points(resultado$centers, cex=2, col=10, pch=19)
